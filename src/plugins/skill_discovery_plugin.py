"""
Advanced Skill Discovery Plugin for Super Alita.

Implements:
- Proposer-Agent-Evaluator (PAE) pipeline for skill evolution
- Monte Carlo Tree Search (MCTS) for skill optimization
- Darwin-GÃ¶del genealogy tracking for skill lineage
- Fitness-based selection and evolution
"""

import ast
import asyncio
import json
import logging
import math
import random
import uuid
from dataclasses import dataclass, field
from datetime import UTC, datetime, timedelta
from typing import Any, Optional

import numpy as np

from src.core.genealogy import trace_birth, trace_fitness
from src.core.neural_atom import create_skill_atom
from src.core.plugin_interface import PluginInterface

logger = logging.getLogger(__name__)


@dataclass
class SkillProposal:
    """A proposed skill for evaluation."""

    id: str
    name: str
    description: str
    code: str
    parent_skills: list[str] = field(default_factory=list)
    proposer: str = "unknown"
    confidence: float = 0.5
    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class SkillEvaluation:
    """Evaluation result for a skill."""

    skill_id: str
    skill_name: str
    performance_score: float
    safety_score: float
    usefulness_score: float
    execution_time: float
    success: bool
    errors: list[str] = field(default_factory=list)
    feedback: str = ""
    test_cases_passed: int = 0
    total_test_cases: int = 0


@dataclass
class MCTSNode:
    """Monte Carlo Tree Search node for skill evolution."""

    skill_id: str
    skill_code: str
    parent: Optional["MCTSNode"] = None
    children: list["MCTSNode"] = field(default_factory=list)
    visits: int = 0
    total_reward: float = 0.0
    fitness_score: float = 0.0
    is_expanded: bool = False
    depth: int = 0

    def ucb1_value(self, exploration_constant: float = 1.414) -> float:
        """Calculate UCB1 value for node selection."""
        if self.visits == 0:
            return float("inf")

        exploitation = self.total_reward / self.visits
        if self.parent and self.parent.visits > 0:
            exploration = exploration_constant * math.sqrt(
                math.log(self.parent.visits) / self.visits
            )
        else:
            exploration = 0.0

        return exploitation + exploration

    def add_child(self, child_node: "MCTSNode") -> None:
        """Add a child node."""
        child_node.parent = self
        child_node.depth = self.depth + 1
        self.children.append(child_node)

    def update_fitness(self, reward: float) -> None:
        """Update node statistics with new reward."""
        self.visits += 1
        self.total_reward += reward
        self.fitness_score = self.total_reward / self.visits


@dataclass
class PAECycle:
    """Perceive-Act-Evolve cycle state."""

    cycle_id: str
    phase: str  # "perceive", "act", "evolve"
    perception_data: dict[str, Any] = field(default_factory=dict)
    action_taken: str | None = None
    evolution_result: dict[str, Any] | None = None
    fitness_score: float = 0.0
    timestamp: datetime = field(default_factory=lambda: datetime.now(UTC))
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def overall_score(self) -> float:
        """Calculate overall skill score."""
        if not self.success:
            return 0.0

        # Weighted combination of different scores
        score = (
            self.performance_score * 0.4
            + self.safety_score * 0.3
            + self.usefulness_score * 0.3
        )

        # Bonus for passing test cases
        if self.total_test_cases > 0:
            test_bonus = (self.test_cases_passed / self.total_test_cases) * 0.2
            score += test_bonus

        return min(1.0, score)


class SkillProposer:
    """Proposes new skills based on context and existing skills."""

    def __init__(self):
        self.proposal_templates = [
            "combine_skills",
            "specialize_skill",
            "generalize_skill",
            "optimize_skill",
            "adapt_skill",
        ]

    async def propose_skill(
        self,
        context: dict[str, Any],
        existing_skills: list[str],
        failed_attempts: list[str] = None,
    ) -> SkillProposal:
        """Propose a new skill based on context."""

        # PILOT-IN-THE-LOOP: Consult LLM for strategic skill proposal
        pilot_proposal = await self._ask_pilot_for_skill_proposal(
            context, existing_skills, failed_attempts or []
        )

        if pilot_proposal:
            logger.info(f"Pilot proposed skill: {pilot_proposal.name}")
            return pilot_proposal

        # Fallback to traditional random approach if Pilot fails
        logger.warning("Pilot consultation failed, using traditional skill proposal")
        proposal_type = random.choice(self.proposal_templates)

        if proposal_type == "combine_skills" and len(existing_skills) >= 2:
            return await self._propose_combined_skill(existing_skills, context)
        if proposal_type == "specialize_skill" and existing_skills:
            return await self._propose_specialized_skill(existing_skills, context)
        if proposal_type == "generalize_skill" and existing_skills:
            return await self._propose_generalized_skill(existing_skills, context)
        if proposal_type == "optimize_skill" and existing_skills:
            return await self._propose_optimized_skill(existing_skills, context)
        return await self._propose_adaptive_skill(context, failed_attempts or [])

    async def _propose_combined_skill(
        self, skills: list[str], context: dict[str, Any]
    ) -> SkillProposal:
        """Propose a skill that combines two existing skills."""

        skill1, skill2 = random.sample(skills, 2)

        name = f"combined_{skill1}_{skill2}"
        description = f"Combines the capabilities of {skill1} and {skill2} for enhanced functionality"

        # Generate simple combination code
        code = f'''
def {name.replace("-", "_")}(input_data, **kwargs):
    """Combined skill: {description}"""

    # Apply first skill
    intermediate_result = apply_skill("{skill1}", input_data, **kwargs)

    # Apply second skill to the result
    final_result = apply_skill("{skill2}", intermediate_result, **kwargs)

    return final_result
'''

        return SkillProposal(
            id=str(uuid.uuid4()),
            name=name,
            description=description,
            code=code.strip(),
            parent_skills=[skill1, skill2],
            proposer="skill_combiner",
            confidence=0.7,
            metadata={"proposal_type": "combination"},
        )

    async def _propose_specialized_skill(
        self, skills: list[str], context: dict[str, Any]
    ) -> SkillProposal:
        """Propose a specialized version of an existing skill."""

        base_skill = random.choice(skills)
        domain = context.get("domain", "general")

        name = f"specialized_{base_skill}_{domain}"
        description = (
            f"Specialized version of {base_skill} optimized for {domain} domain"
        )

        code = f'''
def {name.replace("-", "_")}(input_data, **kwargs):
    """Specialized skill: {description}"""

    # Domain-specific preprocessing
    if "{domain}" in str(input_data).lower():
        # Apply domain-specific optimizations
        processed_input = optimize_for_domain(input_data, "{domain}")
    else:
        processed_input = input_data

    # Apply base skill with specialization
    result = apply_skill("{base_skill}", processed_input,
                        domain="{domain}", **kwargs)

    # Domain-specific postprocessing
    final_result = postprocess_for_domain(result, "{domain}")

    return final_result
'''

        return SkillProposal(
            id=str(uuid.uuid4()),
            name=name,
            description=description,
            code=code.strip(),
            parent_skills=[base_skill],
            proposer="skill_specializer",
            confidence=0.6,
            metadata={"proposal_type": "specialization", "domain": domain},
        )

    async def _propose_generalized_skill(
        self, skills: list[str], context: dict[str, Any]
    ) -> SkillProposal:
        """Propose a generalized version of an existing skill."""

        base_skill = random.choice(skills)

        name = f"generalized_{base_skill}"
        description = (
            f"Generalized version of {base_skill} that works across multiple domains"
        )

        code = f'''
def {name.replace("-", "_")}(input_data, **kwargs):
    """Generalized skill: {description}"""

    # Detect input type/domain automatically
    detected_domain = detect_domain(input_data)

    # Apply adaptive preprocessing
    processed_input = adaptive_preprocess(input_data, detected_domain)

    # Apply base skill with adaptive parameters
    result = apply_skill("{base_skill}", processed_input,
                        adaptive=True, **kwargs)

    # Adaptive postprocessing
    final_result = adaptive_postprocess(result, detected_domain)

    return final_result
'''

        return SkillProposal(
            id=str(uuid.uuid4()),
            name=name,
            description=description,
            code=code.strip(),
            parent_skills=[base_skill],
            proposer="skill_generalizer",
            confidence=0.5,
            metadata={"proposal_type": "generalization"},
        )

    async def _propose_optimized_skill(
        self, skills: list[str], context: dict[str, Any]
    ) -> SkillProposal:
        """Propose an optimized version of an existing skill."""

        base_skill = random.choice(skills)

        name = f"optimized_{base_skill}"
        description = f"Performance-optimized version of {base_skill}"

        code = f'''
def {name.replace("-", "_")}(input_data, **kwargs):
    """Optimized skill: {description}"""

    # Performance optimizations
    if isinstance(input_data, (list, tuple)) and len(input_data) > 100:
        # Use batch processing for large inputs
        results = []
        batch_size = 50
        for i in range(0, len(input_data), batch_size):
            batch = input_data[i:i+batch_size]
            batch_result = apply_skill("{base_skill}", batch, **kwargs)
            results.extend(batch_result)
        return results
    else:
        # Use caching for small inputs
        cache_key = hash(str(input_data) + str(kwargs))
        cached_result = get_from_cache(cache_key)
        if cached_result is not None:
            return cached_result

        result = apply_skill("{base_skill}", input_data, **kwargs)
        cache_result(cache_key, result)
        return result
'''

        return SkillProposal(
            id=str(uuid.uuid4()),
            name=name,
            description=description,
            code=code.strip(),
            parent_skills=[base_skill],
            proposer="skill_optimizer",
            confidence=0.8,
            metadata={"proposal_type": "optimization"},
        )

    async def _propose_adaptive_skill(
        self, context: dict[str, Any], failed_attempts: list[str]
    ) -> SkillProposal:
        """Propose a completely new adaptive skill."""

        task_type = context.get("task_type", "general")

        name = f"adaptive_{task_type}_skill_{random.randint(1000, 9999)}"
        description = f"Adaptive skill for {task_type} tasks with learning capabilities"

        code = f'''
def {name.replace("-", "_")}(input_data, **kwargs):
    """Adaptive skill: {description}"""

    # Adaptive behavior based on input characteristics
    input_size = len(str(input_data))

    if input_size < 100:
        # Simple processing for small inputs
        return simple_process(input_data, **kwargs)
    elif input_size < 1000:
        # Medium complexity processing
        return medium_process(input_data, **kwargs)
    else:
        # Complex processing for large inputs
        return complex_process(input_data, **kwargs)
'''

        return SkillProposal(
            id=str(uuid.uuid4()),
            name=name,
            description=description,
            code=code.strip(),
            parent_skills=[],
            proposer="skill_creator",
            confidence=0.4,
            metadata={"proposal_type": "adaptive", "task_type": task_type},
        )


class SkillEvaluator:
    """Evaluates proposed skills for performance, safety, and usefulness."""

    def __init__(self):
        self.test_environments = {}
        self.safety_checkers = [
            self._check_code_safety,
            self._check_execution_safety,
            self._check_resource_usage,
        ]

    async def evaluate_skill(
        self, proposal: SkillProposal, test_cases: list[dict[str, Any]] = None
    ) -> SkillEvaluation:
        """Evaluate a skill proposal comprehensively."""

        # PILOT-IN-THE-LOOP: Consult LLM for strategic skill evaluation
        pilot_evaluation = await self._ask_pilot_for_skill_evaluation(
            proposal, test_cases or []
        )

        if pilot_evaluation:
            return pilot_evaluation

        # Fallback to traditional evaluation if Pilot fails
        start_time = datetime.now(UTC)
        errors = []

        try:
            # Safety evaluation
            safety_score = await self._evaluate_safety(proposal, errors)

            # Performance evaluation
            performance_score = await self._evaluate_performance(
                proposal, test_cases or [], errors
            )

            # Usefulness evaluation
            usefulness_score = await self._evaluate_usefulness(proposal, errors)

            # Test case evaluation
            test_results = await self._run_test_cases(
                proposal, test_cases or [], errors
            )

            execution_time = (datetime.now(UTC) - start_time).total_seconds()

            success = len(errors) == 0 and safety_score > 0.5

            feedback = self._generate_feedback(
                proposal, safety_score, performance_score, usefulness_score, errors
            )

            return SkillEvaluation(
                skill_id=proposal.id,
                skill_name=proposal.name,
                performance_score=performance_score,
                safety_score=safety_score,
                usefulness_score=usefulness_score,
                execution_time=execution_time,
                success=success,
                errors=errors,
                feedback=feedback,
                test_cases_passed=test_results.get("passed", 0),
                total_test_cases=test_results.get("total", 0),
                metadata={
                    "proposal_type": proposal.metadata.get("proposal_type", "unknown"),
                    "parent_skills": proposal.parent_skills,
                    "confidence": proposal.confidence,
                },
            )

        except Exception as e:
            execution_time = (datetime.now(UTC) - start_time).total_seconds()
            errors.append(f"Evaluation error: {e!s}")

            return SkillEvaluation(
                skill_id=proposal.id,
                skill_name=proposal.name,
                performance_score=0.0,
                safety_score=0.0,
                usefulness_score=0.0,
                execution_time=execution_time,
                success=False,
                errors=errors,
                feedback=f"Evaluation failed: {e!s}",
            )

    async def _evaluate_safety(
        self, proposal: SkillProposal, errors: list[str]
    ) -> float:
        """Evaluate the safety of a skill."""

        safety_scores = []

        for checker in self.safety_checkers:
            try:
                score = await checker(proposal)
                safety_scores.append(score)
            except Exception as e:
                errors.append(f"Safety check error: {e!s}")
                safety_scores.append(0.0)

        return sum(safety_scores) / len(safety_scores) if safety_scores else 0.0

    async def _check_code_safety(self, proposal: SkillProposal) -> float:
        """Check code for dangerous patterns."""

        dangerous_patterns = [
            "import os",
            "import subprocess",
            "import sys",
            "exec(",
            "eval(",
            "__import__",
            "open(",
            "file(",
            "input(",
            "rm ",
            "del ",
            "remove(",
        ]

        code_lower = proposal.code.lower()
        dangerous_count = sum(
            1 for pattern in dangerous_patterns if pattern in code_lower
        )

        # Penalize dangerous patterns
        safety_score = max(0.0, 1.0 - (dangerous_count * 0.3))

        return safety_score

    async def _check_execution_safety(self, proposal: SkillProposal) -> float:
        """Check if code can be safely parsed."""

        try:
            # Try to parse the code
            ast.parse(proposal.code)
            return 1.0
        except SyntaxError:
            return 0.0
        except Exception:
            return 0.5

    async def _check_resource_usage(self, proposal: SkillProposal) -> float:
        """Check for potential resource usage issues."""

        # Look for potentially expensive operations
        expensive_patterns = [
            "while True:",
            "for _ in range(",
            "recursive",
            "infinite",
            "*.+*",
            "heavy",
            "large",
        ]

        code_lower = proposal.code.lower()
        expensive_count = sum(
            1 for pattern in expensive_patterns if pattern in code_lower
        )

        # Moderate penalty for potentially expensive operations
        resource_score = max(0.3, 1.0 - (expensive_count * 0.2))

        return resource_score

    async def _evaluate_performance(
        self,
        proposal: SkillProposal,
        test_cases: list[dict[str, Any]],
        errors: list[str],
    ) -> float:
        """Evaluate skill performance."""

        # Basic performance metrics
        code_length = len(proposal.code)
        complexity_score = max(0.0, 1.0 - (code_length / 1000))  # Prefer shorter code

        # Check for performance-oriented patterns
        performance_patterns = [
            "cache",
            "optimize",
            "batch",
            "efficient",
            "fast",
            "quick",
            "speed",
            "performance",
        ]

        code_lower = proposal.code.lower()
        performance_indicators = sum(
            1 for pattern in performance_patterns if pattern in code_lower
        )

        pattern_score = min(1.0, performance_indicators * 0.2)

        # Combine scores
        performance_score = (complexity_score * 0.7) + (pattern_score * 0.3)

        return performance_score

    async def _evaluate_usefulness(
        self, proposal: SkillProposal, errors: list[str]
    ) -> float:
        """Evaluate the potential usefulness of a skill."""

        # Usefulness indicators
        usefulness_patterns = [
            "process",
            "analyze",
            "transform",
            "generate",
            "optimize",
            "enhance",
            "improve",
            "solve",
        ]

        description_lower = proposal.description.lower()
        code_lower = proposal.code.lower()

        usefulness_indicators = sum(
            1
            for pattern in usefulness_patterns
            if pattern in description_lower or pattern in code_lower
        )

        # Base usefulness score
        base_score = min(1.0, usefulness_indicators * 0.15)

        # Bonus for combining multiple skills
        combination_bonus = min(0.3, len(proposal.parent_skills) * 0.1)

        # Bonus for clear, descriptive naming
        naming_bonus = 0.1 if len(proposal.name.split("_")) > 2 else 0.0

        usefulness_score = base_score + combination_bonus + naming_bonus

        return min(1.0, usefulness_score)

    async def _run_test_cases(
        self,
        proposal: SkillProposal,
        test_cases: list[dict[str, Any]],
        errors: list[str],
    ) -> dict[str, int]:
        """Run test cases for the skill."""

        if not test_cases:
            # Generate simple test cases
            test_cases = [
                {"input": "test", "expected_type": str},
                {"input": [1, 2, 3], "expected_type": (list, tuple)},
                {"input": {"key": "value"}, "expected_type": dict},
            ]

        passed = 0
        total = len(test_cases)

        for test_case in test_cases:
            try:
                # This is a simplified test runner
                # In practice, you'd execute the skill safely
                test_case.get("input")
                expected_type = test_case.get("expected_type", object)

                # Mock execution result
                if isinstance(expected_type, type):
                    passed += 1  # Assume success for now
                else:
                    passed += 1

            except Exception as e:
                errors.append(f"Test case failed: {e!s}")

        return {"passed": passed, "total": total}

    def _generate_feedback(
        self,
        proposal: SkillProposal,
        safety_score: float,
        performance_score: float,
        usefulness_score: float,
        errors: list[str],
    ) -> str:
        """Generate human-readable feedback for the skill."""

        feedback_parts = []

        # Overall assessment
        overall_score = (safety_score + performance_score + usefulness_score) / 3
        if overall_score > 0.8:
            feedback_parts.append("Excellent skill proposal with high quality.")
        elif overall_score > 0.6:
            feedback_parts.append("Good skill proposal with minor issues.")
        elif overall_score > 0.4:
            feedback_parts.append(
                "Acceptable skill proposal with several areas for improvement."
            )
        else:
            feedback_parts.append(
                "Poor skill proposal requiring significant improvements."
            )

        # Specific feedback
        if safety_score < 0.5:
            feedback_parts.append(
                "Safety concerns detected - code may contain risky patterns."
            )

        if performance_score < 0.5:
            feedback_parts.append(
                "Performance could be improved - consider optimization techniques."
            )

        if usefulness_score < 0.5:
            feedback_parts.append(
                "Usefulness is limited - consider more practical applications."
            )

        # Error feedback
        if errors:
            feedback_parts.append(f"Errors encountered: {'; '.join(errors[:3])}")

        return " ".join(feedback_parts)


class SkillDiscoveryPlugin(PluginInterface):
    """
    Skill Discovery Plugin implementing PAE (Proposer-Agent-Evaluator) pipeline.

    Automatically discovers and evolves new skills through iterative
    proposal, testing, and evaluation cycles.
    """

    def __init__(self):
        super().__init__()
        self._name = "skill_discovery_plugin"
        self.proposer = SkillProposer()
        self.evaluator = SkillEvaluator()
        self.active_skills: dict[str, Any] = {}
        self.skill_proposals: dict[str, SkillProposal] = {}
        self.skill_evaluations: dict[str, SkillEvaluation] = {}
        self.discovery_task: asyncio.Task | None = None
        self.evolution_arena: Any | None = None
        self.discovery_stats = {
            "proposals_generated": 0,
            "skills_evaluated": 0,
            "skills_accepted": 0,
            "skills_rejected": 0,
        }

    @property
    def name(self) -> str:
        return self._name

    @property
    def description(self) -> str:
        return "Discovers and evolves agent skills through PAE pipeline"

    async def setup(self, event_bus, store, config: dict[str, Any]) -> None:
        await super().setup(event_bus, store, config)

        # Initialize Gemini client for LLM integration
        try:
            from src.core.gemini_pilot import GeminiPilotClient

            self.gemini_client = GeminiPilotClient()
            logger.info("Gemini client initialized for skill discovery")
        except Exception as e:
            logger.warning(f"Failed to initialize Gemini client: {e}")
            self.gemini_client = None

        # Initialize evolution arena
        self.get_config(
            "base_skills",
            [
                "data_processing",
                "text_analysis",
                "pattern_recognition",
                "optimization",
                "problem_solving",
                "learning",
            ],
        )

        # Initialize MCTS evolution components (placeholder for future implementation)
        self.mcts_root: MCTSNode | None = None
        self.mcts_depth_limit = 5
        self.exploration_constant = 1.414

        # PAE cycle tracking
        self.current_pae_cycle: PAECycle | None = None
        self.pae_history: list[PAECycle] = []

        # Evolution arena placeholder
        # self.evolution_arena = await create_skill_evolution_arena(
        #     base_skills=base_skills,
        #     max_iterations=self.get_config("evolution_iterations", 500)
        # )
        self.evolution_arena = None  # Placeholder for future implementation

    async def start(self) -> None:
        await super().start()

        # Subscribe to events
        await self.subscribe("skill_proposal", self._handle_skill_proposal)
        await self.subscribe("skill_evaluation", self._handle_skill_evaluation_result)
        await self.subscribe("planning", self._handle_planning_event)
        await self.subscribe("system", self._handle_system_event)

        # Start discovery task
        discovery_interval = self.get_config("discovery_interval_minutes", 30)
        self.discovery_task = self.add_task(
            self._periodic_skill_discovery(discovery_interval * 60)
        )

    async def shutdown(self) -> None:
        if self.discovery_task:
            self.discovery_task.cancel()

    async def _handle_skill_proposal(self, event) -> None:
        """Handle external skill proposals."""

        proposal = SkillProposal(
            id=str(uuid.uuid4()),
            name=event.skill_name,
            description=event.skill_description,
            code=event.skill_code,
            parent_skills=getattr(event, "parent_skills", []),
            proposer=event.proposer,
            confidence=getattr(event, "confidence", 0.5),
        )

        await self._evaluate_and_potentially_accept_skill(proposal)

    async def _handle_skill_evaluation_result(self, event) -> None:
        """Handle skill evaluation results."""

        skill_name = event.skill_name

        # Update skill fitness in genealogy
        trace_fitness(f"skill:{skill_name}", event.performance_score)

        # Store evaluation
        if hasattr(event, "skill_id") and event.skill_id in self.skill_proposals:
            evaluation = SkillEvaluation(
                skill_id=event.skill_id,
                skill_name=skill_name,
                performance_score=event.performance_score,
                safety_score=getattr(event, "safety_score", 0.8),
                usefulness_score=getattr(event, "usefulness_score", 0.6),
                execution_time=getattr(event, "execution_time", 0.0),
                success=event.success,
                feedback=getattr(event, "feedback", ""),
            )

            self.skill_evaluations[event.skill_id] = evaluation

    async def _handle_planning_event(self, event) -> None:
        """Handle planning events to discover needed skills."""

        if hasattr(event, "goal"):
            # Extract potential skill needs from goal
            context = {
                "goal": event.goal,
                "task_type": self._infer_task_type(event.goal),
                "domain": getattr(event, "domain", "general"),
            }

            # Consider proposing skills for this goal
            await self._propose_contextual_skills(context)

    async def _handle_system_event(self, event) -> None:
        """Handle system events for skill discovery triggers."""

        if event.message == "skill_discovery_requested":
            await self._trigger_skill_discovery()
        elif event.message == "evolution_requested":
            await self._trigger_evolution()

    async def _periodic_skill_discovery(self, interval_seconds: float) -> None:
        """Periodic skill discovery process."""

        while self.is_running:
            try:
                await asyncio.sleep(interval_seconds)
                await self._trigger_skill_discovery()
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Error in skill discovery: {e}")
                await asyncio.sleep(60)

    async def _trigger_skill_discovery(self) -> None:
        """Trigger skill discovery process."""

        # Get current context
        context = await self._get_discovery_context()

        # Generate proposals
        num_proposals = self.get_config("proposals_per_cycle", 3)
        for _ in range(num_proposals):
            proposal = await self.proposer.propose_skill(
                context=context,
                existing_skills=list(self.active_skills.keys()),
                failed_attempts=self._get_recent_failed_skills(),
            )

            await self._evaluate_and_potentially_accept_skill(proposal)

    async def _trigger_evolution(self) -> None:
        """Trigger evolutionary skill discovery with MCTS and PAE integration."""

        # Start a new PAE cycle
        cycle_id = str(uuid.uuid4())
        self.current_pae_cycle = PAECycle(cycle_id=cycle_id, phase="perceive")

        await self.emit_event(
            "pae_cycle",
            cycle_id=cycle_id,
            phase="perceive",
            perception_data={"trigger": "evolution_request"},
        )

        # Perceive: Analyze current skill landscape
        perception_data = await self._perceive_skill_landscape()
        self.current_pae_cycle.perception_data = perception_data
        self.current_pae_cycle.phase = "act"

        # Act: Run MCTS evolution
        mcts_proposals = await self._run_mcts_evolution(num_iterations=50)

        # Also run traditional evolution if available
        traditional_proposals = []
        if self.evolution_arena:
            evolved_solutions = await self.evolution_arena.evolve(num_generations=3)
            for i, solution in enumerate(evolved_solutions[:3]):
                proposal = await self._solution_to_skill_proposal(solution, i)
                traditional_proposals.append(proposal)

        all_proposals = mcts_proposals + traditional_proposals
        self.current_pae_cycle.action_taken = (
            f"generated_{len(all_proposals)}_proposals"
        )
        self.current_pae_cycle.phase = "evolve"

        # Evolve: Evaluate and select best proposals
        evolution_results = []
        for proposal in all_proposals:
            evaluation = await self._evaluate_skill_proposal(proposal)
            evolution_results.append(
                {
                    "proposal_id": proposal.id,
                    "fitness": evaluation.performance_score,
                    "accepted": evaluation.performance_score > 0.6,
                }
            )

            if evaluation.performance_score > 0.6:
                await self._accept_skill(proposal, evaluation)

        # Complete PAE cycle
        self.current_pae_cycle.evolution_result = {
            "proposals_generated": len(all_proposals),
            "proposals_accepted": sum(1 for r in evolution_results if r["accepted"]),
            "average_fitness": (
                np.mean([r["fitness"] for r in evolution_results])
                if evolution_results
                else 0.0
            ),
        }
        self.current_pae_cycle.fitness_score = self.current_pae_cycle.evolution_result[
            "average_fitness"
        ]

        # Archive completed cycle
        self.pae_history.append(self.current_pae_cycle)
        if len(self.pae_history) > 100:  # Keep last 100 cycles
            self.pae_history = self.pae_history[-100:]

        await self.emit_event(
            "pae_cycle",
            cycle_id=cycle_id,
            phase="evolve",
            evolution_result=self.current_pae_cycle.evolution_result,
            fitness_score=self.current_pae_cycle.fitness_score,
        )

        self.current_pae_cycle = None

    async def _perceive_skill_landscape(self) -> dict[str, Any]:
        """Perceive the current skill landscape for evolution guidance."""
        landscape = {
            "total_skills": len(self.skill_repository),
            "active_skills": len(
                [s for s in self.skill_repository.values() if s.get("active", True)]
            ),
            "recent_evaluations": len(
                [
                    e
                    for e in self.skill_evaluations.values()
                    if e.skill_id in [s for s in self.skill_repository.keys()]
                ]
            ),
            "fitness_distribution": [],
        }

        # Analyze fitness distribution
        fitness_scores = [e.performance_score for e in self.skill_evaluations.values()]
        if fitness_scores:
            landscape["fitness_distribution"] = {
                "mean": np.mean(fitness_scores),
                "std": np.std(fitness_scores),
                "min": np.min(fitness_scores),
                "max": np.max(fitness_scores),
            }

        return landscape

    async def _solution_to_skill_proposal(
        self, solution: dict[str, Any], index: int
    ) -> SkillProposal:
        """Convert evolution solution to skill proposal."""

        skills = solution.get("skills", [])
        depth = solution.get("depth", 0)

        name = f"evolved_skill_{index}_{datetime.now(UTC).strftime('%H%M%S')}"
        description = f"Evolved skill combining: {', '.join(skills[:3])}"

        # Generate code based on evolved skills
        code = f'''
def {name}(input_data, **kwargs):
    """Evolved skill: {description}"""

    result = input_data

    # Apply evolved skill sequence
'''

        for skill in skills[:3]:  # Limit to 3 skills to avoid complexity
            code += f'    result = apply_skill("{skill}", result, **kwargs)\n'

        code += "    return result"

        return SkillProposal(
            id=str(uuid.uuid4()),
            name=name,
            description=description,
            code=code,
            parent_skills=skills[:3],
            proposer="evolution_arena",
            confidence=0.6,
            metadata={
                "proposal_type": "evolution",
                "evolution_depth": depth,
                "solution_index": index,
            },
        )

    async def _evaluate_and_potentially_accept_skill(
        self, proposal: SkillProposal
    ) -> None:
        """Evaluate a skill proposal and potentially accept it."""

        self.skill_proposals[proposal.id] = proposal
        self.discovery_stats["proposals_generated"] += 1

        # Evaluate the skill
        evaluation = await self.evaluator.evaluate_skill(proposal)
        self.skill_evaluations[proposal.id] = evaluation
        self.discovery_stats["skills_evaluated"] += 1

        # Emit evaluation event
        await self.emit_event(
            "skill_evaluation",
            skill_id=proposal.id,
            skill_name=proposal.name,
            performance_score=evaluation.performance_score,
            safety_score=evaluation.safety_score,
            usefulness_score=evaluation.usefulness_score,
            execution_time=evaluation.execution_time,
            success=evaluation.success,
            feedback=evaluation.feedback,
            overall_score=evaluation.overall_score,
        )

        # Accept if meets criteria
        acceptance_threshold = self.get_config("acceptance_threshold", 0.6)
        if evaluation.overall_score >= acceptance_threshold:
            await self._accept_skill(proposal, evaluation)
        else:
            self.discovery_stats["skills_rejected"] += 1

    async def _accept_skill(
        self, proposal: SkillProposal, evaluation: SkillEvaluation
    ) -> None:
        """Accept a skill into the active skill set."""

        # Create skill atom
        atom = await create_skill_atom(
            store=self.store,
            skill_name=proposal.name,
            skill_obj={
                "code": proposal.code,
                "description": proposal.description,
                "evaluation": evaluation,
                "metadata": proposal.metadata,
            },
            parent_skills=proposal.parent_skills,
            metadata={
                "proposer": proposal.proposer,
                "confidence": proposal.confidence,
                "overall_score": evaluation.overall_score,
                "acceptance_time": datetime.now(UTC).isoformat(),
            },
        )

        # Add to active skills
        self.active_skills[proposal.name] = atom
        self.discovery_stats["skills_accepted"] += 1

        # Track in genealogy
        trace_birth(
            key=f"skill:{proposal.name}",
            node_type="skill",
            birth_event="skill_acceptance",
            parent_keys=[f"skill:{parent}" for parent in proposal.parent_skills],
            metadata={
                "proposer": proposal.proposer,
                "overall_score": evaluation.overall_score,
                "proposal_type": proposal.metadata.get("proposal_type", "unknown"),
            },
        )

        # Emit acceptance event
        await self.emit_event(
            "skill_accepted",
            skill_name=proposal.name,
            skill_description=proposal.description,
            parent_skills=proposal.parent_skills,
            overall_score=evaluation.overall_score,
            proposer=proposal.proposer,
        )

        print(
            f"Accepted new skill: {proposal.name} (score: {evaluation.overall_score:.2f})"
        )

    async def _get_discovery_context(self) -> dict[str, Any]:
        """Get context for skill discovery."""

        return {
            "active_skill_count": len(self.active_skills),
            "recent_evaluations": len(self.skill_evaluations),
            "domain": "general",
            "task_type": "adaptive",
            "timestamp": datetime.now(UTC).isoformat(),
        }

    def _infer_task_type(self, goal: str) -> str:
        """Infer task type from goal description."""

        goal_lower = goal.lower()

        if any(word in goal_lower for word in ["analyze", "analysis", "study"]):
            return "analysis"
        if any(word in goal_lower for word in ["generate", "create", "build"]):
            return "generation"
        if any(word in goal_lower for word in ["optimize", "improve", "enhance"]):
            return "optimization"
        if any(word in goal_lower for word in ["learn", "understand", "comprehend"]):
            return "learning"
        return "general"

    def _get_recent_failed_skills(self) -> list[str]:
        """Get recently failed skill attempts."""

        failed_skills = []
        cutoff_time = datetime.now(UTC) - timedelta(hours=24)

        for evaluation in self.skill_evaluations.values():
            proposal = self.skill_proposals.get(evaluation.skill_id)
            if (
                proposal
                and proposal.created_at > cutoff_time
                and not evaluation.success
            ):
                failed_skills.append(proposal.name)

        return failed_skills[-10:]  # Last 10 failures

    async def _propose_contextual_skills(self, context: dict[str, Any]) -> None:
        """Propose skills based on specific context."""

        # This could be enhanced with more sophisticated context analysis
        if context.get("task_type") == "analysis":
            proposal = await self.proposer.propose_skill(
                context={"domain": "analysis", **context},
                existing_skills=list(self.active_skills.keys()),
            )
            await self._evaluate_and_potentially_accept_skill(proposal)

    async def get_discovery_stats(self) -> dict[str, Any]:
        """Get skill discovery statistics."""

        recent_evaluations = [
            eval
            for eval in self.skill_evaluations.values()
            if (
                datetime.now(UTC)
                - datetime.fromisoformat(
                    self.skill_proposals[eval.skill_id].created_at.isoformat()
                )
            ).days
            < 7
        ]

        avg_score = 0.0
        if recent_evaluations:
            avg_score = sum(eval.overall_score for eval in recent_evaluations) / len(
                recent_evaluations
            )

        return {
            **self.discovery_stats,
            "active_skills": len(self.active_skills),
            "pending_proposals": len(self.skill_proposals)
            - len(self.skill_evaluations),
            "recent_average_score": avg_score,
            "acceptance_rate": (
                self.discovery_stats["skills_accepted"]
                / max(self.discovery_stats["skills_evaluated"], 1)
            ),
        }

    async def export_skills(self, filepath: str) -> None:
        """Export discovered skills to JSON file."""

        export_data = {
            "active_skills": {
                name: {
                    "description": skill.default_value.get("description", ""),
                    "code": skill.default_value.get("code", ""),
                    "metadata": skill.lineage_metadata,
                }
                for name, skill in self.active_skills.items()
            },
            "discovery_stats": await self.get_discovery_stats(),
            "recent_proposals": [
                {
                    "name": prop.name,
                    "description": prop.description,
                    "proposer": prop.proposer,
                    "confidence": prop.confidence,
                    "evaluation": (
                        self.skill_evaluations.get(prop.id, {}).overall_score
                        if prop.id in self.skill_evaluations
                        else None
                    ),
                }
                for prop in list(self.skill_proposals.values())[-10:]
            ],
            "export_timestamp": datetime.now(UTC).isoformat(),
        }

        with open(filepath, "w") as f:
            json.dump(export_data, f, indent=2, default=str)

    # ============ MCTS Evolution Methods ============

    async def _run_mcts_evolution(
        self, num_iterations: int = 100
    ) -> list[SkillProposal]:
        """
        Run Monte Carlo Tree Search for skill evolution.

        Implements the four phases: Selection, Expansion, Simulation, Backpropagation.
        """
        if not self.mcts_root:
            # Initialize root with a base skill
            base_skill_code = "def base_function():\n    '''Base function for evolution'''\n    return True"
            self.mcts_root = MCTSNode(
                skill_id="root", skill_code=base_skill_code, depth=0
            )

        best_proposals = []

        for iteration in range(num_iterations):
            # Phase 1: Selection - traverse tree using UCB1
            node = self._select_node(self.mcts_root)

            # Phase 2: Expansion - add new child nodes
            if not node.is_expanded and node.depth < self.mcts_depth_limit:
                await self._expand_node(node)

            # Phase 3: Simulation - evaluate the node
            reward = await self._simulate_skill(node)

            # Phase 4: Backpropagation - update ancestors
            self._backpropagate(node, reward)

            # Emit MCTS event
            await self.emit_event(
                "mcts_operation",
                node_id=node.skill_id,
                operation="iteration_complete",
                value=reward,
                visit_count=node.visits,
                depth=node.depth,
                state_representation={"fitness": node.fitness_score},
            )

            # Collect promising proposals every 10 iterations
            if iteration % 10 == 0:
                promising_nodes = self._get_promising_nodes(min_visits=5)
                for node in promising_nodes:
                    if node.fitness_score > 0.7:  # High fitness threshold
                        proposal = SkillProposal(
                            id=str(uuid.uuid4()),
                            name=f"mcts_evolved_{node.skill_id}",
                            description=f"MCTS evolved skill (fitness: {node.fitness_score:.2f})",
                            code=node.skill_code,
                            proposer="mcts_evolution",
                            confidence=node.fitness_score,
                        )
                        best_proposals.append(proposal)

        return best_proposals[:5]  # Return top 5 proposals

    def _select_node(self, root: MCTSNode) -> MCTSNode:
        """Select the most promising node using UCB1."""
        current = root

        while current.children and current.is_expanded:
            best_child = max(
                current.children, key=lambda c: c.ucb1_value(self.exploration_constant)
            )
            current = best_child

        return current

    async def _expand_node(self, node: MCTSNode) -> None:
        """Expand a node by generating child skill variations."""
        node.is_expanded = True

        # Generate several variations of the current skill
        variations = await self._generate_skill_variations(node.skill_code, count=3)

        for i, variation in enumerate(variations):
            child = MCTSNode(
                skill_id=f"{node.skill_id}_child_{i}",
                skill_code=variation,
                depth=node.depth + 1,
            )
            node.add_child(child)

    async def _generate_skill_variations(
        self, base_code: str, count: int = 3
    ) -> list[str]:
        """Generate variations of a skill through code mutations."""
        variations = []

        for i in range(count):
            # Simple mutations: add comments, change variable names, etc.
            mutated_code = base_code

            # Add randomness
            if random.random() < 0.3:
                mutated_code += f"\n    # Mutation {i}: Enhanced functionality"

            if random.random() < 0.2:
                mutated_code = mutated_code.replace("def ", "def enhanced_")

            variations.append(mutated_code)

        return variations

    async def _simulate_skill(self, node: MCTSNode) -> float:
        """Simulate execution of a skill and return fitness score."""
        try:
            # Create a temporary skill evaluation
            evaluation = await self._evaluate_skill_proposal(
                SkillProposal(
                    id=node.skill_id,
                    name=f"simulation_{node.skill_id}",
                    description="MCTS simulation",
                    code=node.skill_code,
                    proposer="mcts_simulation",
                )
            )

            # Calculate composite fitness
            fitness = (
                evaluation.performance_score * 0.4
                + evaluation.safety_score * 0.3
                + evaluation.usefulness_score * 0.3
            )

            return max(0.0, min(1.0, fitness))  # Clamp to [0, 1]

        except Exception as e:
            self.log("warning", f"MCTS simulation failed for node {node.skill_id}: {e}")
            return 0.0

    def _backpropagate(self, node: MCTSNode, reward: float) -> None:
        """Backpropagate reward up the tree."""
        current = node
        while current:
            current.update_fitness(reward)
            current = current.parent

    def _get_promising_nodes(self, min_visits: int = 1) -> list[MCTSNode]:
        """Get all nodes with sufficient visits and good fitness."""
        promising = []

        def collect_nodes(node: MCTSNode):
            if node.visits >= min_visits:
                promising.append(node)
            for child in node.children:
                collect_nodes(child)

        if self.mcts_root:
            collect_nodes(self.mcts_root)

        return sorted(promising, key=lambda n: n.fitness_score, reverse=True)

    # ==================== PILOT-MECH MODEL METHODS ====================

    async def _ask_pilot_for_skill_proposal(
        self,
        context: dict[str, Any],
        existing_skills: list[str],
        failed_attempts: list[str],
    ) -> SkillProposal | None:
        """
        Consult the LLM (Pilot) for strategic skill proposal.

        This is the core of the Pilot-Mech model: the framework gathers context
        and the LLM proposes intelligent skills based on strategic analysis.
        """
        try:
            # Gather rich context for skill proposal
            task_description = context.get("task_description", "General task")
            goal_description = context.get("goal_description", "Unknown goal")

            # Get relevant skill memories
            if hasattr(self, "_memory") and self._memory:
                try:
                    context_query = (
                        f"Skills for task: {task_description}. Goal: {goal_description}"
                    )
                    query_embedding = await asyncio.wait_for(
                        self._memory.embed_text([context_query]), timeout=15.0
                    )
                    relevant_memories = await self.store.attention(
                        query_embedding[0], top_k=5
                    )
                except (TimeoutError, Exception):
                    relevant_memories = []
            else:
                relevant_memories = []

            # Build the skill proposal prompt
            prompt = self._build_skill_proposal_prompt(
                task_description=task_description,
                goal_description=goal_description,
                existing_skills=existing_skills,
                failed_attempts=failed_attempts,
                relevant_memories=relevant_memories,
                context=context,
            )

            # Consult the Pilot with timeout protection
            try:
                # Use moderate temperature for creative yet strategic proposals
                llm_response = await asyncio.wait_for(
                    self._generate_llm_response(
                        prompt, temperature=0.6, max_tokens=800
                    ),
                    timeout=60.0,
                )

                # Parse the Pilot's skill proposal
                skill_proposal = await self._parse_pilot_skill_proposal(
                    llm_response, context
                )

                if skill_proposal:
                    # Record the strategic proposal for genealogy
                    proposal_metadata = {
                        "pilot_generated": True,
                        "existing_skills_count": len(existing_skills),
                        "failed_attempts_count": len(failed_attempts),
                        "context_used": bool(relevant_memories),
                    }
                    skill_proposal.metadata.update(proposal_metadata)

                    return skill_proposal

            except TimeoutError:
                logger.warning("Pilot skill proposal timed out")
                return None
            except Exception as e:
                logger.error(f"Error during Pilot skill proposal: {e}")
                return None

        except Exception as e:
            logger.error(f"Critical error in _ask_pilot_for_skill_proposal: {e}")
            return None

        return None

    async def _ask_pilot_for_skill_evaluation(
        self, proposal: SkillProposal, test_cases: list[dict[str, Any]]
    ) -> SkillEvaluation | None:
        """
        Consult the LLM (Pilot) for strategic skill evaluation.

        The Pilot analyzes the skill holistically and provides strategic assessment.
        """
        try:
            # Build evaluation prompt with rich context
            prompt = self._build_skill_evaluation_prompt(proposal, test_cases)

            # Consult the Pilot with timeout protection
            try:
                # Use lower temperature for precise evaluation
                llm_response = await asyncio.wait_for(
                    self._generate_llm_response(
                        prompt, temperature=0.3, max_tokens=600
                    ),
                    timeout=45.0,
                )

                # Parse the Pilot's evaluation
                skill_evaluation = await self._parse_pilot_skill_evaluation(
                    llm_response, proposal
                )

                if skill_evaluation:
                    # Mark as Pilot-evaluated
                    skill_evaluation.metadata = skill_evaluation.metadata or {}
                    skill_evaluation.metadata.update(
                        {"pilot_evaluated": True, "evaluation_method": "strategic_llm"}
                    )

                    return skill_evaluation

            except TimeoutError:
                logger.warning(f"Pilot skill evaluation timed out for {proposal.name}")
                return None
            except Exception as e:
                logger.error(f"Error during Pilot skill evaluation: {e}")
                return None

        except Exception as e:
            logger.error(f"Critical error in _ask_pilot_for_skill_evaluation: {e}")
            return None

        return None

    def _build_skill_proposal_prompt(
        self,
        task_description: str,
        goal_description: str,
        existing_skills: list[str],
        failed_attempts: list[str],
        relevant_memories: list,
        context: dict[str, Any],
    ) -> str:
        """
        Construct the Cognitive Contract prompt for strategic skill proposal.

        This is the master template that ensures the Pilot (LLM) receives
        structured, comprehensive context for intelligent skill creation.
        """
        import time

        # Generate correlation ID for this proposal
        correlation_id = f"skill-proposal-{int(time.time())}"

        # Format existing skills (limit to prevent prompt bloat)
        existing_skills_section = "\nexisting_skills:"
        if existing_skills:
            for skill in existing_skills[:10]:  # Limit to top 10
                existing_skills_section += f'\n  - "{skill}"'
        else:
            existing_skills_section += '\n  - "No existing skills available."'

        # Format failed attempts
        failed_attempts_section = "\nfailed_attempts:"
        if failed_attempts:
            for attempt in failed_attempts[:5]:  # Limit to last 5
                failed_attempts_section += f'\n  - "{attempt}"'
        else:
            failed_attempts_section += '\n  - "No previous failures recorded."'

        # Format memories with proper truncation
        memory_section = ""
        if relevant_memories:
            memory_section = "\nrelevant_skill_patterns:"
            for key, score in relevant_memories[:3]:
                atom = self.store.get(key)
                if atom:
                    content_summary = (
                        str(atom.value)[:150] + "..."
                        if len(str(atom.value)) > 150
                        else str(atom.value)
                    )
                    memory_section += f'\n  - key: "{key}"'
                    memory_section += f"\n    similarity_score: {score:.4f}"
                    memory_section += f'\n    content_summary: "{content_summary}"'
        else:
            memory_section = (
                '\nrelevant_skill_patterns:\n  - "No relevant patterns found."'
            )

        # Build the Cognitive Contract prompt
        prompt = f"""# COGNITIVE CONTRACT: Strategic Skill Proposal
# AGENT: Super Alita
# CORRELATION ID: {correlation_id}
# TIMESTAMP: {datetime.now(UTC).isoformat()}

# --- DIRECTIVES AND BINDING PROTOCOLS ---
# ROLE: You are the Skill Innovation Core of the Super Alita agent. Your function is to propose new capabilities that advance toward goals.
# BINDING DIRECTIVE 1: You MUST analyze all provided context to identify capability gaps and propose a strategic skill.
# BINDING DIRECTIVE 2: Your response MUST be a single, valid JSON object, adhering to the 'SkillProposal' schema. No other text is permitted.
# BINDING DIRECTIVE 3: Your proposed skill must be immediately implementable and strategically valuable.

# --- SECTION 1: MISSION CONTEXT ---
mission_context:
  primary_task: "{task_description}"
  ultimate_goal: "{goal_description}"

# --- SECTION 2: CURRENT CAPABILITY INVENTORY ---
capability_inventory:{existing_skills_section}

# --- SECTION 3: LEARNING CONTEXT ---
learning_context:{failed_attempts_section}

# --- SECTION 4: RELEVANT SKILL PATTERNS (Neural Context) ---
# Results from NeuralStore.attention() query for similar skill creation contexts.{memory_section}

# --- SECTION 5: TASK AND REQUIRED OUTPUT SCHEMA ---
task:
  description: "Analyze the mission context and capability gaps. Propose a new skill that would significantly advance progress toward the goal. The skill should either combine existing capabilities in novel ways, overcome past failures, or introduce entirely new functionality that is strategically valuable."
  required_output_schema: {{
    "title": "SkillProposal",
    "type": "object",
    "properties": {{
      "name": {{"type": "string", "description": "Clear, descriptive name for the skill"}},
      "description": {{"type": "string", "description": "Detailed explanation of what the skill does and why it's valuable"}},
      "code": {{"type": "string", "description": "Complete Python function implementation"}},
      "reasoning": {{"type": "string", "description": "Strategic justification for why this skill addresses current needs"}},
      "confidence": {{"type": "number", "minimum": 0.0, "maximum": 1.0}},
      "proposal_type": {{"type": "string", "enum": ["combine", "specialize", "generalize", "optimize", "adaptive", "novel"]}}
    }},
    "required": ["name", "description", "code", "reasoning", "confidence", "proposal_type"]
  }}

# --- END OF CONTRACT ---"""

        return prompt

    def _build_skill_evaluation_prompt(
        self, proposal: SkillProposal, test_cases: list[dict[str, Any]]
    ) -> str:
        """
        Construct the Cognitive Contract prompt for strategic skill evaluation.

        This is the master template that ensures the Pilot (LLM) receives
        structured context for holistic skill assessment.
        """
        import time

        # Generate correlation ID for this evaluation
        correlation_id = f"skill-eval-{int(time.time())}"

        # Format test cases with proper structure
        test_cases_section = "\ntest_cases_context:"
        if test_cases:
            for i, test_case in enumerate(
                test_cases[:3]
            ):  # Limit to 3 for prompt efficiency
                test_cases_section += f"\n  - test_{i + 1}:"
                test_cases_section += f'\n    description: "{test_case.get("description", "No description")}"'
                test_cases_section += f'\n    input_type: "{type(test_case.get("input", "unknown")).__name__}"'
                test_cases_section += f'\n    expected_behavior: "{test_case.get("expected_output", "N/A")}"'
        else:
            test_cases_section += '\n  - "No test cases provided for evaluation."'

        # Build the Cognitive Contract prompt
        prompt = f"""# COGNITIVE CONTRACT: Strategic Skill Evaluation
# AGENT: Super Alita
# CORRELATION ID: {correlation_id}
# TIMESTAMP: {datetime.now(UTC).isoformat()}

# --- DIRECTIVES AND BINDING PROTOCOLS ---
# ROLE: You are the Quality Assurance Core of the Super Alita agent. Your function is to evaluate proposed skills across multiple strategic dimensions.
# BINDING DIRECTIVE 1: You MUST analyze the skill holistically, considering safety, performance, utility, and strategic value.
# BINDING DIRECTIVE 2: Your response MUST be a single, valid JSON object, adhering to the 'SkillEvaluation' schema. No other text is permitted.
# BINDING DIRECTIVE 3: Your evaluation must be both rigorous and strategically informed.

# --- SECTION 1: SKILL SPECIFICATION ---
skill_specification:
  name: "{proposal.name}"
  description: "{proposal.description}"
  proposer: "{proposal.proposer}"
  confidence: {proposal.confidence:.3f}
  proposal_type: "{proposal.metadata.get("proposal_type", "unknown")}"

# --- SECTION 2: IMPLEMENTATION ANALYSIS ---
implementation_code: |
{proposal.code}

# --- SECTION 3: VALIDATION CONTEXT ---{test_cases_section}

# --- SECTION 4: TASK AND REQUIRED OUTPUT SCHEMA ---
task:
  description: "Evaluate this skill across four critical dimensions: (1) Safety - Is the code secure and safe to execute? (2) Performance - How efficient and scalable is the implementation? (3) Usefulness - How valuable is this skill for achieving agent goals? (4) Quality - Is the code well-structured and maintainable? Provide an overall recommendation."
  evaluation_dimensions:
    - safety: "Security risks, potential for harmful execution, input validation"
    - performance: "Efficiency, scalability, resource usage, algorithmic complexity"
    - usefulness: "Strategic value, goal alignment, capability advancement"
    - quality: "Code structure, maintainability, error handling, best practices"
  required_output_schema: {{
    "title": "SkillEvaluation",
    "type": "object",
    "properties": {{
      "safety_score": {{"type": "number", "minimum": 0.0, "maximum": 1.0}},
      "performance_score": {{"type": "number", "minimum": 0.0, "maximum": 1.0}},
      "usefulness_score": {{"type": "number", "minimum": 0.0, "maximum": 1.0}},
      "overall_success": {{"type": "boolean"}},
      "feedback": {{"type": "string", "description": "Detailed evaluation summary"}},
      "reasoning": {{"type": "string", "description": "Strategic rationale for scores"}},
      "recommended_action": {{"type": "string", "enum": ["accept", "reject", "modify"]}}
    }},
    "required": ["safety_score", "performance_score", "usefulness_score", "overall_success", "feedback", "reasoning", "recommended_action"]
  }}

# --- END OF CONTRACT ---"""

        return prompt

    async def _generate_llm_response(
        self, prompt: str, temperature: float = 0.7, max_tokens: int = 500
    ) -> str:
        """
        Generate LLM response with proper error handling.
        In production, this interfaces with the actual LLM service.
        """
        # Use actual Gemini client integration
        if self.gemini_client:
            try:
                response = await self.gemini_client.generate_content(prompt)
                return response.text if hasattr(response, "text") else str(response)
            except Exception as e:
                logger.error(f"Gemini client call failed: {e}")
                # Fallback to mock response for resilience

        # Fallback mock response if Gemini client unavailable
        import json

        if "skill proposal" in prompt:
            mock_response = {
                "name": "pilot_strategic_skill",
                "description": "A strategically designed skill proposed by the Pilot",
                "code": "def pilot_strategic_skill(input_data, **kwargs):\\n    return f'Pilot processed: {input_data}'",
                "reasoning": "This skill addresses the strategic gap identified in the current capability set",
                "confidence": 0.85,
                "proposal_type": "adaptive",
            }
        else:
            # Evaluation response
            mock_response = {
                "safety_score": 0.9,
                "performance_score": 0.8,
                "usefulness_score": 0.85,
                "overall_success": True,
                "feedback": "Well-structured skill with strategic value",
                "reasoning": "Pilot assessment shows high potential for goal achievement",
                "recommended_action": "accept",
            }

        return json.dumps(mock_response)

    async def _parse_pilot_skill_proposal(
        self, llm_response: str, context: dict[str, Any]
    ) -> SkillProposal | None:
        """
        Parse the Pilot's skill proposal response.
        """
        try:
            import json

            proposal_data = json.loads(llm_response)

            required_fields = ["name", "description", "code"]
            if not all(field in proposal_data for field in required_fields):
                logger.error(
                    f"Missing required fields in Pilot proposal: {proposal_data.keys()}"
                )
                return None

            # Create the skill proposal
            skill_proposal = SkillProposal(
                id=str(uuid.uuid4()),
                name=proposal_data["name"],
                description=proposal_data["description"],
                code=proposal_data["code"],
                proposer="pilot_llm",
                confidence=proposal_data.get("confidence", 0.8),
                metadata={
                    "proposal_type": proposal_data.get(
                        "proposal_type", "pilot_adaptive"
                    ),
                    "pilot_reasoning": proposal_data.get(
                        "reasoning", "No reasoning provided"
                    ),
                    "context_goal": context.get("goal_description", "Unknown"),
                },
            )

            return skill_proposal

        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.error(f"Failed to parse Pilot skill proposal: {e}")
            return None

    async def _parse_pilot_skill_evaluation(
        self, llm_response: str, proposal: SkillProposal
    ) -> SkillEvaluation | None:
        """
        Parse the Pilot's skill evaluation response.
        """
        try:
            import json

            eval_data = json.loads(llm_response)

            required_fields = [
                "safety_score",
                "performance_score",
                "usefulness_score",
                "overall_success",
            ]
            if not all(field in eval_data for field in required_fields):
                logger.error(
                    f"Missing required fields in Pilot evaluation: {eval_data.keys()}"
                )
                return None

            # Create the skill evaluation
            skill_evaluation = SkillEvaluation(
                skill_id=proposal.id,
                skill_name=proposal.name,
                performance_score=eval_data["performance_score"],
                safety_score=eval_data["safety_score"],
                usefulness_score=eval_data["usefulness_score"],
                execution_time=0.1,  # Pilot evaluation is fast
                success=eval_data["overall_success"],
                errors=[],
                feedback=eval_data.get("feedback", "Pilot evaluation completed"),
                test_cases_passed=0,  # Pilot does strategic evaluation, not test execution
                total_test_cases=0,
                metadata={
                    "pilot_reasoning": eval_data.get(
                        "reasoning", "No reasoning provided"
                    ),
                    "recommended_action": eval_data.get("recommended_action", "accept"),
                    "evaluation_method": "pilot_strategic",
                },
            )

            return skill_evaluation

        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.error(f"Failed to parse Pilot skill evaluation: {e}")
            return None

    # ==================== COGNITIVE CONTRACT ALIASES ====================

    def _build_skill_proposal_cognitive_contract(
        self,
        task_description: str,
        goal_description: str,
        existing_skills: list[str],
        failed_attempts: list[str],
        relevant_memories: list,
        context: dict[str, Any],
    ) -> str:
        """
        Alias for _build_skill_proposal_prompt to match Cognitive Contract naming convention.
        """
        return self._build_skill_proposal_prompt(
            task_description,
            goal_description,
            existing_skills,
            failed_attempts,
            relevant_memories,
            context,
        )

    def _build_skill_evaluation_cognitive_contract(
        self, proposal, test_cases: list[dict[str, Any]]
    ) -> str:
        """
        Alias for _build_skill_evaluation_prompt to match Cognitive Contract naming convention.
        """
        return self._build_skill_evaluation_prompt(proposal, test_cases)
